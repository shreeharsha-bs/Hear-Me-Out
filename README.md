# Hear Me Out: Exploring Conversational AI Through Speech-to-Speech Models
You can interact with Hear Me Out at this link: https://testing-moshi--hearmeout-web-dev.modal.run/

**Hear Me Out** is an interactive evaluation and bias discovery platform for speech-to-speech conversational AI. These speech-to-speech models process spoken language directly from audio, without first converting it to text. They promise more natural, expressive, and emotionally aware interactions by retaining prosody, intonation, and other vocal cues throughout the conversation.


---
The Hear Me Out block diagram
<img width="1648" alt="hearmeout-BD" src="https://github.com/user-attachments/assets/b282ad4a-354f-4452-ada2-59fafae65629" />
---


## Features

**Hear Me Out** enables users to experience interactions with conversational models in ways that aren't typically accessible with regular benchmarking systems. Key features include:

- **Speech-to-Speech Models**: Users can choose from a variety of models that retain vocal cues like prosody and intonation.
- **Real-Time Voice Conversion**: Step into someone else’s voice – literally – and investigate how conversational AI systems interpret and respond to various speaker identities and expressions.
- **Side-by-Side Comparisons**: Ask a question with your own voice, then re-ask using a transformed voice. Compare the AI’s responses to observe differences in tone, phrasing, or behavior.
- **Insights Through Data**: Visualize metrics like speech rate, sentiment analysis, and more. 
<img width="1381" alt="Screenshot 2025-03-31 at 13 19 18" src="https://github.com/user-attachments/assets/42c5cd60-0fe1-4e58-b198-ff12698e3b3a" />

Through this immersive experience, we hope users will gain insights into identity, voice, and AI behavior. Ultimately, we aim to surface meaningful questions and inspire future research that promotes fairness and inclusivity with **Hear Me Out**.

---


In the demo video, we explore the **Moshi** speech-to-speech model and its responses:

<video controls width="640">
  <source src="assets/IS_st_KTH_Hear-Me-Out-4th_draft.mp4" type="video/mp4">
</video>

### Example 1: Emotional Awareness

Notice how the model disambiguates between inputs with levity and frustration, correctly reflecting the speaker's emotional state in its responses. This distinction adds a more human-like quality to the interaction.

### Example 2: Voice Conversion

By applying voice transformations, we simulate how the model might respond to different speaker characteristics. While the differences in these responses are more subtle and inconsistent under repetition, hearing oneself in another voice opens up new perspectives.
